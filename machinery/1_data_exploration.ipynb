{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "conda_tensorflow2_p36",
      "language": "python",
      "name": "conda_tensorflow2_p36"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "1_data_exploration.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGKJzihRhfhS"
      },
      "source": [
        "# Fault Detection of Machinery by Sound\n",
        "## Mar 9, 2021\n",
        "## by Rihad Variawa, Samira Variawa, Salena Ruhi, Delvin Hada\n",
        "### Artificial Intelligence\n",
        "*Step 1 - Data exploration*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZQkM6Dchfhr"
      },
      "source": [
        "## Preface\n",
        "There are usually very few redondancies on a plant production line of any shop floor. The ability to ensure the highest avaibility of these lines is key to deliver higher ROI, produce better quality products, increase safety levels, reduce environmental impact and waste and in the end... ensure greater customer satisfaction. More and more Ai-based solutions are leveraging sensor data coming from the pieces of machinery on the production line. Today, we are going to use audio recorded in an industrial environment to perform fault detection on indsutrial equipment\n",
        "\n",
        "To achieve this, we are going to explore and leverage the MIMII dataset for fault detection purpose: this is a sound dataset for **M**alfunctioning **I**ndustrial **M**achine **I**nvestigation and **I**nspection (MIMII). It can be downloaded from **[this link](https://zenodo.org/record/3384388#.X2werWgzaTk)** and contains sounds from several types of industrial machines (valves, pumps, fans and slide rails). Our focus will be **fans**\n",
        "\n",
        "This is the first notebook in a series of three:\n",
        "* This notebook will help us get familiar with this kind of data. Sound data are particular time series data and exploring them requires specific approaches\n",
        "* In the next notebook we'll build an autoencoder to discriminate between normal and abnormal sounds\n",
        "* Lastly, we are going to take on a more novel approach in the last part of the work: we are going to transform the sound files into spectrogram images and feed them to an image classifier. We'll use Rekognition to perform this task\n",
        "\n",
        "The two approaches (building a basic autoencoder from scratch and leveraging Rekognition) amounts approximately to the same effort: although the models are in no way comparable it will give us an idea of how much of a kick start we can get when using an applied Ai solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0UsAreNhfht"
      },
      "source": [
        "## Initialization\n",
        "**WARNING**: make sure you run this notebook using an **ml.c5.2xlarge instance** with a **25 GB attached EBS volume** to process the MIMII dataset (the dataset for the industrial fans is a 10 GB archive, reaching 14 GB once unzipped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFDQFXLohfht"
      },
      "source": [
        "### Configuration\n",
        "Remove the **-q** command line parameters if you want to check for potential error messages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym9td10Yhfhu"
      },
      "source": [
        "# Notebook update\n",
        "import sys\n",
        "\n",
        "!pip -q install --upgrade sagemaker\n",
        "if 'librosa' not in list(sys.modules):\n",
        "    !conda install -q -y -c conda-forge librosa\n",
        "\n",
        "# Kernel restart\n",
        "from IPython.core.display import HTML\n",
        "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcTaxi4Phfhw"
      },
      "source": [
        "# Libraries\n",
        "import hashlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "\n",
        "# Sound management\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "\n",
        "sys.path.append('tools')\n",
        "import utils\n",
        "import sound_tools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MphtaYhThfhx"
      },
      "source": [
        "# Initialization\n",
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "plt.style.use('Solarize_Light2')\n",
        "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
        "colors = prop_cycle.by_key()['color']\n",
        "blue, red = colors[1], colors[5]\n",
        "\n",
        "# Paths definition\n",
        "DATA           = os.path.join('data', 'interim')\n",
        "RAW_DATA       = os.path.join('data', 'raw')\n",
        "PROCESSED_DATA = os.path.join('data', 'processed')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQn1jkaQhfhx"
      },
      "source": [
        "### Loading helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR-JcRMzhfhx"
      },
      "source": [
        "### Downloading and unzipping data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni9dsPnahfhy"
      },
      "source": [
        "if not os.path.exists(DATA):\n",
        "    print('Data directory does not exist, creating them.')\n",
        "    os.makedirs(DATA, exist_ok=True)\n",
        "    os.makedirs(RAW_DATA, exist_ok=True)\n",
        "    os.makedirs(PROCESSED_DATA, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKhHzqzghfhz"
      },
      "source": [
        "# Preview dataset downloded and unzipped\n",
        "first_file = os.path.join(DATA, 'fan', 'id_00', 'normal', '00000000.wav')\n",
        "if os.path.exists(first_file):\n",
        "    print('=== Sound files found, no need to download them again. ===')\n",
        "    \n",
        "else:\n",
        "    print('=== Downloading and unzipping the FAN file from the MIMII dataset website (~10 GB) ===')\n",
        "    !wget https://zenodo.org/record/3384388/files/6_dB_fan.zip?download=1 --output-document=/tmp/fan.zip\n",
        "    \n",
        "    # Preview file integrity: computing MD5 hash\n",
        "    original_md5 = '0890f7d3c2fd8448634e69ff1d66dd47'\n",
        "    downloaded_md5 = utils.md5('/tmp/fan.zip')\n",
        "    \n",
        "    # Correct MD5, unzipping archive:\n",
        "    if original_md5 == downloaded_md5:\n",
        "        !unzip -q /tmp/fan.zip -d $DATA\n",
        "        \n",
        "    # Raising exception for an incorrect MD5:\n",
        "    else:\n",
        "        raise Exception('Downloaded file was corrupted, retry the download.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HESHM8oshfhz"
      },
      "source": [
        "### Feature engineering parameters\n",
        "These parameters are used to extract features from sound files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWBe88o9hfhz"
      },
      "source": [
        "n_mels = 64\n",
        "frames = 5\n",
        "n_fft = 2048\n",
        "hop_length = 512\n",
        "power = 2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm3o5AeOhfhz"
      },
      "source": [
        "## Visualization\n",
        "Let's load a normal and an abnormal sound to plot them. Each recording contains **8 channels, one for each microphone** that was used to record the machine sound. For the remaining of this task, **we'll only focus on the recordings of the first microphone**\n",
        "### Wave forms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT1qe5lNhfhz"
      },
      "source": [
        "normal_signal_file = os.path.join(DATA, 'fan', 'id_00', 'normal', '00000100.wav')\n",
        "abnormal_signal_file = os.path.join(DATA, 'fan', 'id_00', 'abnormal', '00000100.wav')\n",
        "normal_signal, sr = sound_tools.load_sound_file(normal_signal_file)\n",
        "abnormal_signal, sr = sound_tools.load_sound_file(abnormal_signal_file)\n",
        "print(f'The signals have a {normal_signal.shape} shape. At {sr} Hz, these are {normal_signal.shape[0]/sr:.0f}s signals')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwpFxwyvhfh0"
      },
      "source": [
        "Let's first visualize the waveplots for these signals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmIpT_5Vhfh1"
      },
      "source": [
        "fig = plt.figure(figsize=(24,6))\n",
        "plt.subplot(1,3,1)\n",
        "librosa.display.waveplot(normal_signal, sr=sr, alpha=0.5, color=blue, linewidth=0.5, label='Machine #id_00 - Normal signal')\n",
        "plt.title('Normal signal')\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "librosa.display.waveplot(abnormal_signal, sr=sr, alpha=0.6, color=red, linewidth=0.5, label='Machine #id_00 - Abnormal signal')\n",
        "plt.title('Abnormal signal')\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "librosa.display.waveplot(abnormal_signal, sr=sr, alpha=0.6, color=red, linewidth=0.5, label='Abnormal signal')\n",
        "librosa.display.waveplot(normal_signal, sr=sr, alpha=0.5, color=blue, linewidth=0.5, label='Normal signal')\n",
        "plt.title('Both signals')\n",
        "\n",
        "fig.suptitle('Machine #id_00 - 2D representation of the wave forms', fontsize=16)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1pRoYTYhfh1"
      },
      "source": [
        "Apart from the larger amplitude of the abnormal signal and some patterns that are more irregular, it's difficult to distinguish between these two signals. Let's listen to them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eQKSpFLhfh2"
      },
      "source": [
        "ipd.Audio(os.path.join(DATA, 'fan', 'id_00', 'normal', '00000003.wav'), rate=sr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsO0vQfZhfh2"
      },
      "source": [
        "ipd.Audio(os.path.join(DATA, 'fan', 'id_00', 'abnormal', '00000003.wav'), rate=sr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYinllMkhfh2"
      },
      "source": [
        "We can hear a small difference. **Let's now have a look in the frequency domain** and see if we can make that difference more obvious..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhXVPI0khfh2"
      },
      "source": [
        "### Short Fourier transform\n",
        "Let's take the Fourier transform of a first time window. Such signals are highly non-stationary (i.e., their statistics change over time). As a consequence, it will be rather meaningless to compute a single Fourier transform over an entire signal. The short-time Fourier transform is obtained by computing the Fourier transform for successive frames in a signal. We can compute it thanks to the **`librosa.stft()`** function that returns a complex-valued matrix D where:\n",
        "* `np.abs(D[f, t])` is the magnitude of frequency bin **f** at frame **t** and\n",
        "* `np.angle(D[f, t])` is the corresponding phase for the same frequency bin **f** at frame **t**.\n",
        "\n",
        "The parameter `n_fft` of this function is the length of the window signal (frame size) while the `hop_length` is the frame increment. Our signals are 10s long: with `n_fft = 2048` and at a sampling rate of 16 kHz, this corresponds to a physical duration of `2048/16000 = 128 ms`. Let's display the FFT of the first 128ms window (by limiting the signal span and by setting a hop length greater than `n_fft`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62aiZPcBhfh2"
      },
      "source": [
        "D_normal = np.abs(librosa.stft(normal_signal[:n_fft], n_fft=n_fft, hop_length=n_fft + 1))\n",
        "D_abnormal = np.abs(librosa.stft(abnormal_signal[:n_fft], n_fft=n_fft, hop_length=n_fft + 1))\n",
        "\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "plt.plot(D_normal, color=blue, alpha=0.6, label='Machine #id_00 - Normal signal')\n",
        "plt.plot(D_abnormal, color=red, alpha=0.6, label='Machine #id_00 - Abnormal signal')\n",
        "plt.title('Fourier transform for the first 64ms window')\n",
        "plt.xlabel('Frequency (Hz)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.legend()\n",
        "plt.xlim(0,200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jla00gnahfh3"
      },
      "source": [
        "### Spectrograms\n",
        "Let's know take the entire signals, separate them in time windows of `hop_length` width, apply a short Fourier transform on each of these windows and plot them on a spectrogram to illustrate these three dimensions:\n",
        "* Frequency (Hz) is now on the vertical axis\n",
        "* Amplitude is shifted from the vertical axis of the previous diagram to the color axis\n",
        "* The horizontal axis represents time\n",
        "\n",
        "The following diagram is a plot of the short Fourier transforms for 20 bins (20 x 128ms = 2560ms which is the span of the horizontal axis) for the first 500 Hz (frequency span on the vertical axis)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9UA780yhfh4"
      },
      "source": [
        "D_normal = np.abs(librosa.stft(normal_signal[:20*n_fft], n_fft=n_fft, hop_length=hop_length))\n",
        "dB_normal = sound_tools.get_magnitude_scale(normal_signal_file)\n",
        "\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "librosa.display.specshow(D_normal, sr=sr, x_axis='time', y_axis='linear', cmap='viridis')\n",
        "plt.title('Machine #id_00 - Normal signal\\nShort Fourier Transform representation of the First 2560ms')\n",
        "plt.ylim(0,500)\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Frequency (Hz)')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLKESxAchfh4"
      },
      "source": [
        "Let's now plot the same spectrogram for the whole signal (10s), a higher frequency range and for both a normal and an abnormal signals. Each spectrogram will have a dimension of `int(160,000 / hop_length) + 1 = 313` bins on the horizontal axis and `n_fft / 2 = 1,024` bins on the vertical axis. Hence, the dimension of the spectrogram for a given sound will be `1,024 x 313`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdl4wC5yhfh4"
      },
      "source": [
        "D_normal = np.abs(librosa.stft(normal_signal, n_fft=n_fft, hop_length=hop_length))\n",
        "D_abnormal = np.abs(librosa.stft(abnormal_signal, n_fft=n_fft, hop_length=hop_length))\n",
        "\n",
        "fig = plt.figure(figsize=(24,6))\n",
        "plt.subplot(1,2,1)\n",
        "librosa.display.specshow(D_normal, sr=sr, x_axis='time', y_axis='linear', cmap='viridis')\n",
        "plt.title('Machine #id_00 - Normal signal')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Frequency (Hz)')\n",
        "plt.colorbar()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "librosa.display.specshow(D_abnormal, sr=sr, x_axis='time', y_axis='linear', cmap='viridis')\n",
        "plt.title('Machine #id_00 - Abnormal signal')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Frequency (Hz)')\n",
        "plt.colorbar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seuo9_MOhfh5"
      },
      "source": [
        "Not much we can see here, mainly because most sounds we can hear or experience as humans, are concentrated in a very small range (both in frequency and amplitude range). Let's take a log scale for both the frequency and the amplitude: for the amplitude, we obtain this by transforming the \"color\" axis to a log scale by converting it to Decibels (which is equivalent to applying a log scale to the sound amplitudes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTiDBPQ0hfh5"
      },
      "source": [
        "dB_normal = sound_tools.get_magnitude_scale(normal_signal_file, n_fft=n_fft, hop_length=hop_length)\n",
        "dB_abnormal = sound_tools.get_magnitude_scale(abnormal_signal_file, n_fft=n_fft, hop_length=hop_length)\n",
        "\n",
        "fig = plt.figure(figsize=(24,6))\n",
        "plt.subplot(1,2,1)\n",
        "librosa.display.specshow(dB_normal, sr=sr, x_axis='time', y_axis='mel', cmap='viridis')\n",
        "plt.title('Machine #id_00 - Normal signal')\n",
        "plt.colorbar(format=\"%+2.f dB\")\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Frequency (Hz)')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "librosa.display.specshow(dB_abnormal, sr=sr, x_axis='time', y_axis='mel', cmap='viridis')\n",
        "plt.title('Machine #id_00 - Abnormal signal')\n",
        "plt.ylabel('Frequency (Hz)')\n",
        "plt.colorbar(format=\"%+2.f dB\")\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Frequency (Hz)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACkoSYTShfh5"
      },
      "source": [
        "### Applying Mel scale\n",
        "The way the Mel scale is constructed is to allow sounds that are at equal distance from each other on this scale, to also *sound* as if they were at equal distance if a human hear them. As a human, our ear is easily able to distinguish two sounds at frequency 250 Hz and 500 Hz. However, we will barely notice any difference between two other sounds emitted at frequency 9750 Hz and 10000 Hz (which are still 250 Hz apart)... This non-linear transformation is also available in the `librosa` library. The following function partitions the frequency scales in bins and transform each of them into the corresponding bin in the Mel scale: this produces a linear transformation matrix to project FFT bins onto Mel-frequency bins. Then it uses this transformation matrix to plot a new spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0CH329nhfh5"
      },
      "source": [
        "normal_mel = librosa.feature.melspectrogram(normal_signal, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
        "normal_S_DB = librosa.power_to_db(normal_mel, ref=np.max)\n",
        "abnormal_mel = librosa.feature.melspectrogram(abnormal_signal, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
        "abnormal_S_DB = librosa.power_to_db(abnormal_mel, ref=np.max)\n",
        "\n",
        "fig = plt.figure(figsize=(24,6))\n",
        "plt.subplot(1,2,1)\n",
        "librosa.display.specshow(normal_S_DB, sr=sr, hop_length=hop_length, x_axis='time', y_axis='mel', cmap='viridis')\n",
        "plt.title('Machine #id_00 - Normal signal')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Frequency (Hz)')\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "librosa.display.specshow(abnormal_S_DB, sr=sr, hop_length=hop_length, x_axis='time', y_axis='mel', cmap='viridis')\n",
        "plt.title('Machine #id_00 - Abnormal signal')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Frequency (Hz)')\n",
        "plt.colorbar(format='%+2.0f dB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRTtrt_9hfh6"
      },
      "source": [
        "frames = 5\n",
        "stride = 1\n",
        "dims = frames * n_mels\n",
        "\n",
        "features_vector_size = normal_S_DB.shape[1] - frames + 1\n",
        "features = np.zeros((features_vector_size, dims), np.float32)\n",
        "for t in range(frames):\n",
        "    features[:, n_mels * t: n_mels * (t + 1)] = normal_S_DB[:, t:t + features_vector_size].T\n",
        "fig = plt.figure(figsize=(24,3))\n",
        "for t in range(frames):\n",
        "    plt.subplot(1, frames, t + 1)\n",
        "    librosa.display.specshow(features[:, n_mels * t: n_mels * (t + 1)].T, sr=sr, hop_length=hop_length, cmap='viridis')\n",
        "    \n",
        "features_vector_size = abnormal_S_DB.shape[1] - frames + 1\n",
        "features = np.zeros((features_vector_size, dims), np.float32)\n",
        "for t in range(frames):\n",
        "    features[:, n_mels * t: n_mels * (t + 1)] = abnormal_S_DB[:, t:t + features_vector_size].T\n",
        "fig = plt.figure(figsize=(24,3))\n",
        "for t in range(frames):\n",
        "    plt.subplot(1, frames, t + 1)\n",
        "    librosa.display.specshow(features[:, n_mels * t: n_mels * (t + 1)].T, sr=sr, hop_length=hop_length, cmap='viridis')\n",
        "    #plt.colorbar(format='%+2.0f dB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWcvqdJZhfh6"
      },
      "source": [
        "The binning process in the frequency domain applied by the Mel transformation yields a more pixelated diagram which is consistent with the lower sound resolution a human hears when compared to a microphone. It's also useful to dilate any features we can try and extract for a deep learning model afterward..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-e5N-bmhfh7"
      },
      "source": [
        "### Conclusion\n",
        "The Mel spectrogram looks like a good candidate to extract interesting features that we could feed to a neural network. We will know build two types of feature extractor based on this analysis and feed them to different type of architectures:\n",
        "1. Extracting features into a tabular dataset that we will feed to an autoencoder neural network: as computed above each raw spectrogram has a shape of `1024 x 313` where the Mel spectrogram will have a shape of `64 x 313`\n",
        "2. Using the spectrograms as an input to feed a computer vision-based architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdkYdkjGnYGT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
