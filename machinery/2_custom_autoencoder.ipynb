{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "conda_tensorflow2_p36",
      "language": "python",
      "name": "conda_tensorflow2_p36"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "2_custom_autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2CQVhEXoFOS"
      },
      "source": [
        "# Fault Detection of Machinery by Sound\n",
        "## Mar 9, 2021\n",
        "## by Rihad Variawa, Samira Variawa, Salena Ruhi, Delvin Hada\n",
        "### Artificial Intelligence\n",
        "*Step 2 - Performing fault detection with an autoencoder architecture*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J11hPTtToFOc"
      },
      "source": [
        "## Introduction\n",
        "Here is [an article](https://towardsdatascience.com/autoencoder-neural-network-for-anomaly-detection-with-unlabeled-dataset-af9051a048) you can go through to dive deeper into what an autoencoder is and how it can be used for anomaly detections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUQ46lsxoFOd"
      },
      "source": [
        "## Initialization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecM10PTToFOd"
      },
      "source": [
        "### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a94PXp97oFOd"
      },
      "source": [
        "# Libraries\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import time\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Other imports\n",
        "from sklearn import metrics\n",
        "from tqdm import tqdm\n",
        "\n",
        "# AWS and SageMaker libraries\n",
        "import sagemaker\n",
        "import boto3\n",
        "from sagemaker.tensorflow import TensorFlow\n",
        "\n",
        "sys.path.append('tools')\n",
        "import sound_tools\n",
        "import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXt_O7RcoFOf"
      },
      "source": [
        "# Initializations\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('Solarize_Light2')\n",
        "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
        "colors = prop_cycle.by_key()['color']\n",
        "blue = colors[1]\n",
        "red = colors[5]\n",
        "\n",
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "sess = sagemaker.Session()\n",
        "role = sagemaker.get_execution_role()\n",
        "\n",
        "# Paths definition\n",
        "DATA           = os.path.join('data', 'interim')\n",
        "RAW_DATA       = os.path.join('data', 'raw')\n",
        "PROCESSED_DATA = os.path.join('data', 'processed')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seSVLrJkoFOf"
      },
      "source": [
        "### Feature engineering parameters\n",
        "These parameters are used to extract features from sound files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4QEFB5ToFOf"
      },
      "source": [
        "n_mels = 64\n",
        "frames = 5\n",
        "n_fft = 1024\n",
        "hop_length = 512\n",
        "power = 2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTxFMXwToFOg"
      },
      "source": [
        "## **Step 1:** Building the datasets\n",
        "\n",
        "### Generate list of sound files and splitting them\n",
        "Generate list of files found in the raw data folder and generate a training dataset from the folders marked for training. We will be training an autoencoder below:\n",
        "\n",
        "* Testing dataset: **1110 signals** including:\n",
        "  * 295 abnormal signals\n",
        "  * 815 normal signals\n",
        "* Training dataset: **3260 signals** only including normal signals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9v2maqyoFOh"
      },
      "source": [
        "# Build the list of normal and abnormal files\n",
        "normal_files, abnormal_files = utils.build_files_list(root_dir=os.path.join(DATA, 'fan'))\n",
        "\n",
        "# Concatenate them to obtain features and label datasets that we can split\n",
        "X = np.concatenate((normal_files, abnormal_files), axis=0)\n",
        "y = np.concatenate((np.zeros(len(normal_files)), np.ones(len(abnormal_files))), axis=0)\n",
        "\n",
        "train_files, test_files, train_labels, test_labels = train_test_split(X, y,\n",
        "                                                                      train_size=0.8,\n",
        "                                                                      random_state=123,\n",
        "                                                                      shuffle=True,\n",
        "                                                                      stratify=y\n",
        "                                                                     )\n",
        "\n",
        "# We will want to reuse this same train/test split for our next experiment in the next notebook\n",
        "dataset = dict({\n",
        "    'train_files': train_files,\n",
        "    'test_files': test_files,\n",
        "    'train_labels': train_labels,\n",
        "    'test_labels': test_labels\n",
        "})\n",
        "\n",
        "for key, values in dataset.items():\n",
        "    fname = os.path.join(PROCESSED_DATA, key + '.txt')\n",
        "    with open(fname, 'w') as f:\n",
        "        for item in values:\n",
        "            f.write(str(item))\n",
        "            f.write('\\n')\n",
        "\n",
        "# We now keep only the normal signals for the train files to train the autoencoder\n",
        "train_files = [f for f in train_files if f not in abnormal_files]\n",
        "train_labels = np.zeros(len(train_files))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zmhFV6-oFOh"
      },
      "source": [
        "### Extracting spectrograms as tabular features\n",
        "Based on the previous data exploration notebook, the training data are generated by computing a spectrogram from each signal and extracting features from these. To build the feature vector (performd in the `generate_dataset()` function), we divide the Mel spectrogram of each signal into several `(=frames)` sliding windows. We then concatenate these windows to assemble a single feature matrix associated to each signal. This tabular-shaped feature will then be fed to an autoencoder down the road"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX6h-eyMoFOh"
      },
      "source": [
        "train_data_location = os.path.join(DATA, 'train_data.pkl')\n",
        "\n",
        "if os.path.exists(train_data_location):\n",
        "    print('Train data already exists, loading from file...')\n",
        "    with open(train_data_location, 'rb') as f:\n",
        "        train_data = pickle.load(f)\n",
        "        \n",
        "else:\n",
        "    train_data = sound_tools.generate_dataset(train_files, n_mels=n_mels, frames=frames, n_fft=n_fft, hop_length=hop_length)\n",
        "    print('Saving training data to disk...')\n",
        "    with open(os.path.join(DATA, 'train_data.pkl'), 'wb') as f:\n",
        "        pickle.dump(train_data, f)\n",
        "    print('Done.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kV9YcfnoFOi"
      },
      "source": [
        "#### S3 buckets preparation\n",
        "We upload the train dataset on S3. We use the default bucket of this SageMaker instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_yyDnSqoFOi"
      },
      "source": [
        "training_input_path = sess.upload_data(os.path.join(DATA, 'train_data.pkl'), key_prefix='training')\n",
        "print(training_input_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7HR6x0qoFOi"
      },
      "source": [
        "## **Step 2:** Creating a TensorFlow model\n",
        "\n",
        "### Define an Estimator\n",
        "We are using the TensorFlow container with script mode. The following script will be used as the entry point of the training container\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38h7mu-roFOj"
      },
      "source": [
        "!pygmentize autoencoder/model.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ljIC47coFOj"
      },
      "source": [
        "tf_estimator = TensorFlow(\n",
        "    base_job_name='sound-outlier-detection',\n",
        "    entry_point='model.py',\n",
        "    source_dir='./autoencoder/',\n",
        "    role=role,\n",
        "    instance_count=1, \n",
        "    instance_type='ml.p3.2xlarge',\n",
        "    framework_version='2.2',\n",
        "    py_version='py37',\n",
        "    hyperparameters={\n",
        "        'epochs': 30,\n",
        "        'batch-size': 512,\n",
        "        'learning-rate': 1e-3,\n",
        "        'n_mels': n_mels,\n",
        "        'frame': frames\n",
        "    },\n",
        "    debugger_hook_config=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1yOY3oioFOj"
      },
      "source": [
        "### Model training\n",
        "At this point, we'll incur some training costs: the training is quite fast (a few minutes) and you can use spot training to reduce the bill by 70%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teIYGqCRoFOk"
      },
      "source": [
        "tf_estimator.fit({'training': training_input_path})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSdE6cYJoFOk"
      },
      "source": [
        "### Model deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQh-QfJCoFOk"
      },
      "source": [
        "We can now deploy our trained model behind a SageMaker endpoint: this endpoint will continue costing you on an hourly basis, don't forget to delete it once your done with this notebook!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUUmO644oFOk"
      },
      "source": [
        "tf_endpoint_name = 'sound-outlier-detection-'+time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
        "tf_predictor = tf_estimator.deploy(\n",
        "    initial_instance_count=1,\n",
        "    instance_type='ml.c5.large',\n",
        "    endpoint_name=tf_endpoint_name\n",
        ")\n",
        "\n",
        "print(f'\\nEndpoint name: {tf_predictor.endpoint_name}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDEQVcCHoFOk"
      },
      "source": [
        "## **Step 3:** Model evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epEgB9J_oFOk"
      },
      "source": [
        "### Apply model on a test dataset\n",
        "Now we will loop through the test dataset and send each test file to the endpoint. As our model is an autoencoder, we will evaluate how good the model is at reconstructing the input. The highest the reconstruction error, the more chance we have to identify a fault"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq1FsFnAoFOl"
      },
      "source": [
        "y_true = test_labels\n",
        "reconstruction_errors = []\n",
        "\n",
        "for index, eval_filename in tqdm(enumerate(test_files), total=len(test_files)):\n",
        "    # Load signal\n",
        "    signal, sr = sound_tools.load_sound_file(eval_filename)\n",
        "\n",
        "    # Extract features from this signal\n",
        "    eval_features = sound_tools.extract_signal_features(\n",
        "        signal, \n",
        "        sr, \n",
        "        n_mels=n_mels, \n",
        "        frames=frames, \n",
        "        n_fft=n_fft, \n",
        "        hop_length=hop_length\n",
        "    )\n",
        "    \n",
        "    # Get predictions from our autoencoder\n",
        "    prediction = tf_predictor.predict(eval_features)['predictions']\n",
        "    \n",
        "    # Estimate the reconstruction error\n",
        "    mse = np.mean(np.mean(np.square(eval_features - prediction), axis=1))\n",
        "    reconstruction_errors.append(mse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbALvmUaoFOl"
      },
      "source": [
        "### Reconstruction error analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF70XNNmoFOl"
      },
      "source": [
        "In the plot below, we can see that the distribution of reconstruction error for normal and abnormal signals differs significantly. However, the overlap we can see between both histograms, means we will have to compromise between precision and recall"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmYZTH0loFOm"
      },
      "source": [
        "data = np.column_stack((range(len(reconstruction_errors)), reconstruction_errors))\n",
        "bin_width = 0.25\n",
        "bins = np.arange(min(reconstruction_errors), max(reconstruction_errors) + bin_width, bin_width)\n",
        "\n",
        "fig = plt.figure(figsize=(12,4))\n",
        "plt.hist(data[y_true==0][:,1], bins=bins, color=blue, alpha=0.6, label='Normal signals', edgecolor='#FFFFFF')\n",
        "plt.hist(data[y_true==1][:,1], bins=bins, color=red, alpha=0.6, label='Abnormal signals', edgecolor='#FFFFFF')\n",
        "plt.xlabel(\"Testing reconstruction error\")\n",
        "plt.ylabel(\"# Samples\")\n",
        "plt.title('Reconstruction error distribution on the testing set', fontsize=16)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLkHbhGgoFOm"
      },
      "source": [
        "Let's explore the recall-precision trade off for a reconstruction error threshold varying between 5.0 and 10.0 (this encompasses most of the overlap we can see above). First, let's visualize how this threshold range separates our signals on a scatter plot of all the testing samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjDua_Y9oFOm"
      },
      "source": [
        "# Set threshold ranges\n",
        "threshold_min = 5.0\n",
        "threshold_max = 10.0\n",
        "threshold_step = 0.50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qotaNxsKoFOm"
      },
      "source": [
        "# Scatter data for normal and abnormal signals\n",
        "normal_x, normal_y = data[y_true==0][:,0], data[y_true==0][:,1]\n",
        "abnormal_x, abnormal_y = data[y_true==1][:,0], data[y_true==1][:,1]\n",
        "x = np.concatenate((normal_x, abnormal_x))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(24,8))\n",
        "plt.scatter(normal_x, normal_y, s=15, color='tab:green', alpha=0.3, label='Normal signals')\n",
        "plt.scatter(abnormal_x, abnormal_y, s=15, color='tab:red', alpha=0.3,   label='Abnormal signals')\n",
        "plt.fill_between(x, threshold_min, threshold_max, alpha=0.1, color='tab:orange', label='Threshold range')\n",
        "plt.hlines([threshold_min, threshold_max], x.min(), x.max(), linewidth=0.5, alpha=0.8, color='tab:orange')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Threshold range exploration', fontsize=16)\n",
        "plt.xlabel('Samples')\n",
        "plt.ylabel('Reconstruction error')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DimTSKYKoFOn"
      },
      "source": [
        "### Confusion matrix analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjedAZTkoFOn"
      },
      "source": [
        "thresholds = np.arange(threshold_min, threshold_max + threshold_step, threshold_step)\n",
        "\n",
        "df = pd.DataFrame(columns=['Signal', 'Ground Truth', 'Prediction', 'Reconstruction Error'])\n",
        "df['Signal'] = test_files\n",
        "df['Ground Truth'] = test_labels\n",
        "df['Reconstruction Error'] = reconstruction_errors\n",
        "\n",
        "FN = []\n",
        "FP = []\n",
        "for th in thresholds:\n",
        "    df.loc[df['Reconstruction Error'] <= th, 'Prediction'] = 0.0\n",
        "    df.loc[df['Reconstruction Error'] > th, 'Prediction'] = 1.0\n",
        "    df = utils.generate_error_types(df)\n",
        "    FN.append(df['FN'].sum())\n",
        "    FP.append(df['FP'].sum())\n",
        "        \n",
        "utils.plot_curves(FP, FN, nb_samples=df.shape[0], threshold_min=threshold_min, threshold_max=threshold_max, threshold_step=threshold_step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s_dgak0oFOn"
      },
      "source": [
        "The above curves, shows us that the best compromise is to use a threshold set around 6.3 to 6.5 for the reconstruction error (assuming we are not looking at minimizing either the false positive or false negatives occurrences)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rQlzgAzoFOn"
      },
      "source": [
        "th = 6.275\n",
        "df.loc[df['Reconstruction Error'] <= th, 'Prediction'] = 0.0\n",
        "df.loc[df['Reconstruction Error'] > th, 'Prediction'] = 1.0\n",
        "df['Prediction'] = df['Prediction'].astype(np.float32)\n",
        "df = utils.generate_error_types(df)\n",
        "tp = df['TP'].sum()\n",
        "tn = df['TN'].sum()\n",
        "fn = df['FN'].sum()\n",
        "fp = df['FP'].sum()\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "df['Ground Truth'] = 1 - df['Ground Truth']\n",
        "df['Prediction'] = 1 - df['Prediction']\n",
        "utils.print_confusion_matrix(confusion_matrix(df['Ground Truth'], df['Prediction']), class_names=['abnormal', 'normal'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEJRuguboFOo"
      },
      "source": [
        "df.to_csv(os.path.join(PROCESSED_DATA, 'results_autoencoder.csv'), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RALDaeCBoFOo"
      },
      "source": [
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "f1_score = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "print(f\"\"\"Basic autoencoder metrics:\n",
        "- Precision: {precision*100:.1f}%\n",
        "- Recall: {recall*100:.1f}%\n",
        "- Accuracy: {accuracy*100:.1f}%\n",
        "- F1 Score: {f1_score*100:.1f}%\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjBoto3WoFOo"
      },
      "source": [
        "## Cleanup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJZ6lnP3oFOp"
      },
      "source": [
        "Finally, we should delete the endpoint before we close this notebook\n",
        "\n",
        "To do so execute the cell below. Alternately, you can navigate to the **Endpoints** tab in the SageMaker console, select the endpoint with the name stored in the variable endpoint_name, and select **Delete** from the **Actions** dropdown menu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVdySOUMoFOp"
      },
      "source": [
        "sess.delete_endpoint(tf_predictor.endpoint_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMMkeEAKoFOp"
      },
      "source": [
        "## Epilogue: model improvement and further exploration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytWRfx-0oFOp"
      },
      "source": [
        "This spectrogram approach requires us to define the spectrogram square dimensions (e.g. the number of Mel cell defined in the data exploration notebook) which is a heuristic. In contrast, deep learning networks with a CNN encoder can learn the best representation to perform the task at hand (fault detection). Further steps to investigate to improve on this first result could be:\n",
        "* Experimenting with several more or less complex autoencoder architectures, training for a longer time, performing hyperparameter tuning with different optimizer, tuning the data preparation sequence (e.g. sound discretization parameters), etc.\n",
        "* Leveraging high resolution spectrograms and feed them to a CNN encoder to uncover the most appropriate representation of the audio\n",
        "* Building end-to-end model architecture with encoder-decoder: they have been known to give good results on waveform datasets\n",
        "* Using deep learning models with multi-context temporal and channel (8 microphones in this case) attention weights\n",
        "* Using time distributed 2D convolution layers to encode features across the 8 channels: these encoded features could then be fed as sequences across time steps to an LSTM or GRU layer. From there, multiplicative sequence attention weights can then be learnt on the output sequence from the RNN layer\n",
        "* Exploring the appropriate image representation for multivariate time series signal that are not waveform: replacing spectrograms with Markov transition fields, recurrence plots or network graphs could then be used to achieve the same goals for non-sound time-based signals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv0jap61sFg0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
